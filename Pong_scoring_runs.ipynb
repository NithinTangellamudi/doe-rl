{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Making scores for all trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RUN IF NOT INSTALLED\n",
    "# !pip install gym\n",
    "# !pip install opencv-python\n",
    "# !pip install gym[atari]\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CHANGE ENVIRONMENT HERE'''\n",
    "''' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ '''\n",
    "env_id = \"PongNoFrameskip-v0\"\n",
    "''' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ '''\n",
    "import math, random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "# import file\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "out_dir = 'outdir/'\n",
    "out_dir_scoring = 'outdir_scoring_rewards/'\n",
    "\n",
    "class CnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CnnDQN, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), volatile=True)\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].data[0]\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action\n",
    "\n",
    "from wrappers import make_atari, wrap_deepmind, wrap_pytorch\n",
    "env    = make_atari(env_id)\n",
    "env    = wrap_deepmind(env)\n",
    "env    = wrap_pytorch(env)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)\n",
    "\n",
    "\n",
    "    \n",
    "def Training(iteration,outfile,risk=\"low\",sample_ratio=\"low\",epsilon_decay=\"low\",buffer_size=\"low\"):\n",
    "    \n",
    "    cur_ckpt_flnm = out_dir + 'outdir_{}_{}_{}_{}/current_{}_{}_{}_{}.ckpt'.format(risk,sample_ratio,epsilon_decay,buffer_size,risk,sample_ratio,epsilon_decay,buffer_size)\n",
    "    cur_model_flnm = out_dir + 'outdir_{}_{}_{}_{}/current_{}_{}_{}_{}.model'.format(risk,sample_ratio,epsilon_decay,buffer_size,risk,sample_ratio,epsilon_decay,buffer_size)\n",
    "\n",
    "    def plot(frame_idx, rewards, losses):\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20,5))\n",
    "#         plt.subplot(131)\n",
    "        plt.title('Reward Over %s Episodes, Average Reward: %.2f' % (frame_idx, np.mean(rewards)))\n",
    "        plt.plot(rewards)\n",
    "        plt.savefig(out_dir_scoring + 'scoring_rewards_{}_{}_{}_{}.png'.format(risk,sample_ratio,epsilon_decay,buffer_size))\n",
    "\n",
    "#         plt.subplot(132)\n",
    "#         plt.title('loss')\n",
    "#         plt.plot(losses)\n",
    "#         plt.savefig(out_dir + 'losses_{}_{}_{}_{}.png'.format(risk,sample_ratio,epsilon_decay,buffer_size))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    from collections import deque\n",
    "\n",
    "    class ReplayBuffer(object):\n",
    "        def __init__(self, capacity):\n",
    "            self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "        def push(self, state, action, reward, next_state, done):\n",
    "            state      = np.expand_dims(state, 0)\n",
    "            next_state = np.expand_dims(next_state, 0)\n",
    "\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "        def sample(self, batch_size):\n",
    "            state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "            return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.buffer)\n",
    "    \n",
    "    current_model = torch.load(cur_model_flnm,map_location=\"cpu\")\n",
    "    current_model.load_state_dict(torch.load(cur_ckpt_flnm,map_location=\"cpu\"))\n",
    "\n",
    "    if USE_CUDA:\n",
    "        current_model = current_model.cuda()\n",
    "\n",
    "    optimizer = optim.Adam(current_model.parameters(), lr=0.00001)\n",
    "\n",
    "    replay_initial = 10000\n",
    "    replay_buffer = ReplayBuffer(10000)\n",
    "\n",
    "    def get_epsilon_by_frame():\n",
    "        epsilon_start = 1.0\n",
    "        epsilon_final = 0.000000001\n",
    "\n",
    "        epsilon_decay_steps = 1000000\n",
    "            \n",
    "        epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay_steps)\n",
    "        plt.plot([epsilon_by_frame(i) for i in range(epsilon_decay_steps)])\n",
    "\n",
    "        return epsilon_by_frame\n",
    "\n",
    "\n",
    "    num_episodes = 100\n",
    "    batch_size = 32\n",
    "    gamma      = 0.99\n",
    "\n",
    "    epsilon_by_frame = get_epsilon_by_frame()\n",
    "\n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    episode_reward = 0\n",
    "    episode=0\n",
    "\n",
    "    state = env.reset()\n",
    "#     env.render()\n",
    "    while episode<num_episodes:\n",
    "        epsilon = epsilon_by_frame(10000000)\n",
    "        action = current_model.act(state, epsilon)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            all_rewards.append(episode_reward)\n",
    "\n",
    "            episode_reward = 0\n",
    "            episode+=1\n",
    "            plot(len(all_rewards), all_rewards, losses)\n",
    "            \n",
    "    for i in all_rewards:\n",
    "        outfile.write(str(iteration) + \" \" + str(i) + \"\\n\")\n",
    "        \n",
    "    np.save(out_dir_scoring+ \"rewards_{}_{}_{}_{}\".format(risk,sample_ratio,epsilon_decay,buffer_size),all_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running 100 trials for each 2^4 setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret(number):\n",
    "    if number == '0':\n",
    "        return \"low\"\n",
    "    return \"high\"\n",
    "\n",
    "\n",
    "outfile = open(\"sasinput.txt\",\"a\")\n",
    "file_2k = open(\"2k_rem.txt\",\"r\")\n",
    "\n",
    "for i in range(10,15):\n",
    "    params = list(file_2k.readline())[:4]\n",
    "    Training(i,outfile,risk=interpret(params[0]),sample_ratio=interpret(params[1]),epsilon_decay=interpret(params[2]),buffer_size=interpret(params[3]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Making SAS input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "out_dir = 'outdir/'\n",
    "out_dir_scoring = 'outdir_scoring_rewards/'\n",
    "\n",
    "def interpret(number):\n",
    "    if number == \"0\":\n",
    "        return \"low\"\n",
    "    return \"high\"\n",
    "\n",
    "outfile = open(\"sas.txt\",\"a\")\n",
    "file_2k_read = open(\"2k_factorial_guide.txt\",\"r\")\n",
    "\n",
    "\n",
    "for i in range(16):\n",
    "    params = list(file_2k_read.readline())[:4]\n",
    "    fname = out_dir_scoring+ \"rewards_\" + interpret(params[0]) + \"_\" + interpret(params[1]) + \"_\" + interpret(params[2]) + \"_\" + interpret(params[3]) + \".npy\"\n",
    "#     print(fname)\n",
    "    data = np.load(fname)\n",
    "    for i in data:\n",
    "        outfile.write(params[0]+\" \"+params[1]+\" \"+params[2]+\" \"+params[3]+\" \" +str(i) + \"\\n\")\n",
    "#     print(data)\n",
    "\n",
    "outfile.close()\n",
    "file_2k_read.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
